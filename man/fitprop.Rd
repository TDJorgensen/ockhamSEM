% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/try-lavaanList.R
\name{fitprop}
\alias{fitprop}
\title{Fit Model to Random Data Patterns}
\usage{
fitprop(
  object,
  use.FitProp = NULL,
  reps = 1000,
  seed = 1234,
  ...,
  rmethod = c("onion", "mcmc", "clustergen"),
  onlypos = FALSE,
  mcmc.args = list(),
  clustergen.args = list(),
  baseline.model = NULL
)
}
\arguments{
\item{object}{an object of class \code{\linkS4class{lavaan}}}

\item{use.FitProp}{Optional object of class \code{\linkS4class{FitProp}}.
Assuming \code{object} is fitted to the same variables, providing this
can save computing time by fitting the model in \code{object} to the same
generated data patterns in \code{use.FitProp}, in which case no
further arguments should be specified (they will be recycled).}

\item{reps}{\code{integer} indicating how many random data patterns to generate}

\item{seed}{\code{integer} passed to \code{\link{set.seed}}}

\item{...}{Additional arguments passed to \code{\link[lavaan]{lavaanList}}}

\item{rmethod}{Optional \code{character} indicating which among a set of
methods to use, which are implemented in the \pkg{ockhamSEM} package
(see \strong{Details}). Required when not specifying a custom
\code{dataFunction=} via \dots}

\item{onlypos}{\code{logical} indicating whether to restrict random data
patterns to be positive manifold (see \strong{Details}).  Ignored when a
custom \code{dataFunction=} is provided via \dots}

\item{mcmc.args}{Optional named \code{list} of arguments that controls
options when \code{rmethod = "mcmc"} (see \strong{Details}).}

\item{clustergen.args}{Optional named \code{list} of arguments that controls
options when \code{rmethod = "onion"} or \code{"clustergen"}
(see \strong{Details}).}

\item{baseline.model}{Optional object of class \code{\linkS4class{lavaan}}
to be passed to \code{\link[lavaan]{fitMeasures}}}
}
\value{
An object of class \code{\linkS4class{FitProp}}, which inherits from
  \code{\linkS4class{lavaan}}.
}
\description{
Estimate a model's fitting propensity to evaluate model complexity
beyond merely degrees of freedom
}
\details{
Inspired by work by Preacher (2003, 2006) and Bonifay & Cai (2017),
this function performs three steps for analyses to assess the fit propensity of competing
structural equation models:

1. Randomly generate correlation (or covariance matrices);
2. Fit models to each correlation matrix; and
3. Save a indices that could be used for evaluating model fit in subsequent summaries.

Conceptually, models that exhibit better fit to such randomly generated data
may have better fit propensity, and are therefore potentially less parsimonious.

Models must be fitted with the \code{\pkg{lavaan}} package (e.g., created
from \code{\link[lavaan]{cfa}}, \code{\link[lavaan]{sem}}, or
\code{\link[lavaan]{lavaan}} functions).
Currently, only models for normal data are supported using ML or ULS estimators.
The underlying \code{\link[lavaan]{lavOptions}} from the fitted
\code{\linkS4class{lavaan}} models will be re-used when fitting the
same model to each random data pattern, which will be saved in the
returned object.
A \code{summary} method is available to summarize results for a single
model, and \code{\link{compareFitProp}} is available for model comparison.

Generation of random correlation matrices is provided using several approaches. The \code{"mcmc"}
algorithm implements a Markov Chain Monte Carlo approach and was ported from Fortran code
in Preacher (2003). For details on the algorithm's actual implementation, see Preacher (2003),
Falk and Muthukrishna (in press), or the source code for the \code{mcmc()} function. If this algorithm
is chosen, \code{mcmc.args} accepts a list that can modify some default settings. In particular,
\code{iter} sets the total number of iterations to run (default = 5000000). If parallel processing
is enabled, this number will be divided among the number of chains. \code{miniter} sets a
minimum number of iterations per chain to avoid many processors leading to too few iterations per
chain (default = 10000). \code{jmpsize} overrides the step size for each update to the candidate
correlation matrix. Smaller step sizes typically lead to more acceptance and may be necessary for
larger correlation matrices (default jump size depends on the number of variables). Though, in
general the MCMC algorithm becomes more difficult to work well with many variables.

The \code{"onion"} method is one approach that relies on work of Joe (2006) and
Lewandowski, Kurowick, and Joe (2009); matrices are generated recursively, one variable at a
time. The onion method is computationally more efficient than the MCMC algorithm. Under the
hood, the \code{\link[clusterGeneration]{genPositiveDefMat}} function in the
\code{\pkg{clusterGeneration}} package is used, with default
arguments of \code{covMethod="onion"}, \code{eta=1}, and \code{rangeVar=c(1,1)}. These arguments ensure that the
Onion method is used, generation is uniform over the space of positive definite matrices
(but see note on positive manifold below), and with unit variances.

An additional option \code{"clustergen"} is provided for direct interface with the \code{\link[clusterGeneration]{genPositiveDefMat}}
function in the \code{\pkg{clusterGeneration}} package. A named list can be passed to \code{clustergen.args} to
override any defaults used by \code{\link[clusterGeneration]{genPositiveDefMat}}, and the user is referred to documentation
for that function. This allows, for example, generation using C-Vines, covariance matrices
(i.e., variables that do not all have unit variances), and several other covariance/correlation
matrix generation techniques.

The \code{onlypos=} argument controls whether correlation matrices are
restricted to have only positive correlations.
The original MCMC algorithm by Preacher (2003, 2006) generated correlation
matrices with positive manifold only (i.e., only positive correlations).
The algorithm is easily changed to allow also negative correlations.
The \code{"onion"} method and any functions from \code{\pkg{clusterGeneration}}
by default generate matrices with both positive and negative correlations.
To obtain matrices with positive manifold only, an ad-hoc correction is
implemented for these latter approaches where the matrix \eqn{R} is transformed:

\deqn{R = (R+1)/2}

To our knowledge, there is no guarantee that this will result in uniform
sampling from the space of all correlation matrices with positive manifold,
yet fit propensity results for some examples are very similar to those
of the MCMC algorithm.
}
\examples{

# Set up a covariance matrix to fit models to
p <- 3 # number of variables
temp_mat <- diag(p) # identity matrix
colnames(temp_mat) <- rownames(temp_mat) <- paste0("V", seq(1, p))

# Define and fit path models using lavaan package
mod1 <- 'V3 ~ V1 + V2
  V1 ~~ 0*V2'
mod2 <- 'V3 ~ V1
  V2 ~ V3'
mod3 <- 'V3 ~ V2
  V2 ~ V1'

fit1 <- sem(mod1, sample.cov=temp_mat, sample.nobs=500)
fit2 <- sem(mod2, sample.cov=temp_mat, sample.nobs=500)
fit3 <- sem(mod3, sample.cov=temp_mat, sample.nobs=500)

## obtain fitting propensity for each model
fp1 <- fitprop(fit1, reps = 1000, onlypos = TRUE)
fp2 <- fitprop(fit2, reps = 1000, onlypos = TRUE)
fp3 <- fitprop(fit3, reps = 1000, onlypos = TRUE)

## summarize 1 model's fitting propensity
summary(fp1, fit.measures = c("srmr","logl","cfi"),
        lower.tail = c(TRUE, FALSE, FALSE), NML = TRUE, UIF = TRUE)

## compare fitting propensity of a set of models
compareFitProp(fp1, fp2, fp3, fit.measures = c("srmr","logl","cfi"),
               lower.tail = c(TRUE, FALSE, FALSE), conf = .95,
               ## compare fit, adjusted for fitting propensity:
               NML = TRUE, UIF = TRUE)


## Factor Models

HS.model <- ' visual  =~ x1 + x2 + x3
              textual =~ x4 + x5 + x6
              speed   =~ x7 + x8 + x9 '
mod.hi <- ' g =~ visual + textual + speed '
mod.bi <- paste0('g =~ x', 1:9)
mod.par <- c(paste0('g =~ 1*x', 1:9), paste0('x', 1:9, ' ~~ evar*x', 1:9))

fit.hi <- cfa(c(HS.model, mod.hi), data = HolzingerSwineford1939,
              std.lv = TRUE, estimator = "uls")
fit.bi <- cfa(c(HS.model, mod.bi), data = HolzingerSwineford1939,
              orthogonal = TRUE, std.lv = TRUE, estimator = "uls")
fit.uni <- cfa(mod.bi, data = HolzingerSwineford1939,
               std.lv = TRUE, estimator = "uls")
fit.par <- cfa(mod.par, data = HolzingerSwineford1939, estimator = "uls")

fp.hi  <- fitprop(fit.hi, reps = 100, onlypos = TRUE,
                  parallel = "multicore", ncpus = 10,
                  baseline.model = fit.par)
fp.bi  <- fitprop(fit.bi, reps = 100, onlypos = TRUE,
                  parallel = "multicore", ncpus = 10,
                  baseline.model = fit.par)
fp.uni <- fitprop(fit.uni, reps = 100, onlypos = TRUE,
                  parallel = "multicore", ncpus = 10,
                  baseline.model = fit.par)
summary(fp.hi, UIF = TRUE)
## can name the models passed to ...
compareFitProp(Higher.Order = fp.hi, Bifactor = fp.bi, Unidimensinoal = fp.uni,
               fit.measures = c("srmr","cfi"), lower.tail = c(TRUE, FALSE),
               conf = .95, UIF = TRUE)

}
\references{
Bonifay, W. E., & Cai, L. (2017). On the complexity of item response theory
models. \emph{Multivariate Behavioral Research, 52}(4), 465--484.
\doi{10.1080/00273171.2017.1309262}

Falk, C. F., & Muthukrishna, M. (in press). Parsimony in model selection:
Tools for assessing fit propensity. \emph{Psychological Methods}.
\doi{10.1037/met0000422}

Lewandowski, D., Kurowicka, D., & Joe, H. (2009). Generating random
correlation matrices based on vines and extended onion method.
\emph{Journal of Multivariate Analysis, 100}(9), 1989--2001.
\doi{10.1016/j.jmva.2009.04.008}

Joe, H. (2006). Generating random correlation matrices based on partial
correlations. \emph{Journal of Multivariate Analysis, 97}(10), 2177--2189.
\doi{10.1016/j.jmva.2005.05.010}

Preacher, K. J. (2003).
\emph{The role of model complexity in the evaluation of structural equation models}
 (Unpublished doctoral dissertation). The Ohio State University:
\url{http://rave.ohiolink.edu/etdc/view?acc_num=osu1054130634}

Preacher, K. J. (2006). Quantifying parsimony in structural equation modeling.
\emph{Multivariate Behavioral Research, 41}(3), 227--259.
\doi{10.1207/s15327906mbr4103_1}
}
\seealso{
\code{\linkS4class{FitProp}}, \code{\link{compareFitProp}}
}
\author{
Terrence D. Jorgensen (University of Amsterdam; \email{TJorgensen314@gmail.com})
}
